{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "res = urlopen(\"https://www.gutenberg.org/files/100/100-0.txt\")\n",
    "if res.status == 200:\n",
    "    text = res.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub(r\"([\\\\.].)\", \" \", text)\n",
    "text = re.sub(r\"[^A-Za-z0-9 ]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "\n",
    "i_to_c = {i: c for i, c in enumerate(chars)}\n",
    "c_to_i = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "encoded = [c_to_i[c] for c in text]\n",
    "sequences = []\n",
    "next_char = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "step = 5\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 62,\n",
       " 17,\n",
       " 24,\n",
       " 34,\n",
       " 0,\n",
       " 21,\n",
       " 38,\n",
       " 36,\n",
       " 48,\n",
       " 21,\n",
       " 34,\n",
       " 29,\n",
       " 2,\n",
       " 34,\n",
       " 62,\n",
       " 37,\n",
       " 56,\n",
       " 38,\n",
       " 4,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 41,\n",
       " 17,\n",
       " 19,\n",
       " 12,\n",
       " 18,\n",
       " 34,\n",
       " 21,\n",
       " 34,\n",
       " 38,\n",
       " 57,\n",
       " 17,\n",
       " 62,\n",
       " 22,\n",
       " 56,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 38,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 2,\n",
       " 27,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 4,\n",
       " 20,\n",
       " 3,\n",
       " 56,\n",
       " 38,\n",
       " 34,\n",
       " 49,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 38,\n",
       " 3,\n",
       " 56,\n",
       " 38,\n",
       " 23,\n",
       " 17,\n",
       " 62,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 48,\n",
       " 56,\n",
       " 34,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 54,\n",
       " 29,\n",
       " 27,\n",
       " 17,\n",
       " 29,\n",
       " 34,\n",
       " 38,\n",
       " 54,\n",
       " 29,\n",
       " 27,\n",
       " 33,\n",
       " 20,\n",
       " 34,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 3,\n",
       " 29,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 5,\n",
       " 29,\n",
       " 3,\n",
       " 21,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 25,\n",
       " 21,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 56,\n",
       " 38,\n",
       " 54,\n",
       " 29,\n",
       " 59,\n",
       " 19,\n",
       " 17,\n",
       " 56,\n",
       " 21,\n",
       " 38,\n",
       " 17,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 62,\n",
       " 38,\n",
       " 12,\n",
       " 54,\n",
       " 62,\n",
       " 21,\n",
       " 56,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 33,\n",
       " 17,\n",
       " 62,\n",
       " 18,\n",
       " 59,\n",
       " 38,\n",
       " 54,\n",
       " 21,\n",
       " 38,\n",
       " 29,\n",
       " 17,\n",
       " 38,\n",
       " 0,\n",
       " 17,\n",
       " 56,\n",
       " 21,\n",
       " 38,\n",
       " 54,\n",
       " 29,\n",
       " 59,\n",
       " 38,\n",
       " 33,\n",
       " 3,\n",
       " 21,\n",
       " 20,\n",
       " 38,\n",
       " 54,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 56,\n",
       " 21,\n",
       " 38,\n",
       " 29,\n",
       " 17,\n",
       " 38,\n",
       " 62,\n",
       " 34,\n",
       " 56,\n",
       " 21,\n",
       " 62,\n",
       " 3,\n",
       " 0,\n",
       " 21,\n",
       " 3,\n",
       " 17,\n",
       " 29,\n",
       " 56,\n",
       " 33,\n",
       " 20,\n",
       " 54,\n",
       " 21,\n",
       " 56,\n",
       " 17,\n",
       " 34,\n",
       " 14,\n",
       " 34,\n",
       " 62,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 17,\n",
       " 48,\n",
       " 38,\n",
       " 19,\n",
       " 54,\n",
       " 27,\n",
       " 38,\n",
       " 0,\n",
       " 17,\n",
       " 12,\n",
       " 27,\n",
       " 38,\n",
       " 3,\n",
       " 21,\n",
       " 38,\n",
       " 37,\n",
       " 3,\n",
       " 14,\n",
       " 34,\n",
       " 38,\n",
       " 3,\n",
       " 21,\n",
       " 38,\n",
       " 54,\n",
       " 33,\n",
       " 54,\n",
       " 27,\n",
       " 38,\n",
       " 17,\n",
       " 62,\n",
       " 38,\n",
       " 62,\n",
       " 34,\n",
       " 48,\n",
       " 56,\n",
       " 34,\n",
       " 38,\n",
       " 3,\n",
       " 21,\n",
       " 38,\n",
       " 48,\n",
       " 29,\n",
       " 59,\n",
       " 34,\n",
       " 62,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 21,\n",
       " 34,\n",
       " 62,\n",
       " 19,\n",
       " 56,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 26,\n",
       " 62,\n",
       " 17,\n",
       " 24,\n",
       " 34,\n",
       " 0,\n",
       " 21,\n",
       " 38,\n",
       " 36,\n",
       " 48,\n",
       " 21,\n",
       " 34,\n",
       " 29,\n",
       " 2,\n",
       " 34,\n",
       " 62,\n",
       " 37,\n",
       " 38,\n",
       " 15,\n",
       " 3,\n",
       " 0,\n",
       " 34,\n",
       " 29,\n",
       " 56,\n",
       " 34,\n",
       " 38,\n",
       " 3,\n",
       " 29,\n",
       " 0,\n",
       " 18,\n",
       " 48,\n",
       " 59,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 33,\n",
       " 3,\n",
       " 21,\n",
       " 20,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 3,\n",
       " 56,\n",
       " 38,\n",
       " 34,\n",
       " 49,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 38,\n",
       " 17,\n",
       " 62,\n",
       " 38,\n",
       " 17,\n",
       " 29,\n",
       " 18,\n",
       " 3,\n",
       " 29,\n",
       " 34,\n",
       " 38,\n",
       " 54,\n",
       " 21,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 38,\n",
       " 48,\n",
       " 21,\n",
       " 34,\n",
       " 29,\n",
       " 2,\n",
       " 34,\n",
       " 62,\n",
       " 37,\n",
       " 38,\n",
       " 62,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 53,\n",
       " 23,\n",
       " 38,\n",
       " 27,\n",
       " 17,\n",
       " 48,\n",
       " 38,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 29,\n",
       " 17,\n",
       " 21,\n",
       " 38,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 3,\n",
       " 29,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 5,\n",
       " 29,\n",
       " 3,\n",
       " 21,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 25,\n",
       " 21,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 56,\n",
       " 38,\n",
       " 27,\n",
       " 17,\n",
       " 48,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 54,\n",
       " 14,\n",
       " 34,\n",
       " 38,\n",
       " 21,\n",
       " 17,\n",
       " 38,\n",
       " 0,\n",
       " 20,\n",
       " 34,\n",
       " 0,\n",
       " 22,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 18,\n",
       " 54,\n",
       " 33,\n",
       " 56,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 21,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 0,\n",
       " 17,\n",
       " 48,\n",
       " 29,\n",
       " 21,\n",
       " 62,\n",
       " 27,\n",
       " 38,\n",
       " 33,\n",
       " 20,\n",
       " 34,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 27,\n",
       " 17,\n",
       " 48,\n",
       " 38,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 2,\n",
       " 34,\n",
       " 23,\n",
       " 17,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 48,\n",
       " 56,\n",
       " 3,\n",
       " 29,\n",
       " 37,\n",
       " 21,\n",
       " 20,\n",
       " 3,\n",
       " 56,\n",
       " 38,\n",
       " 34,\n",
       " 2,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 38,\n",
       " 4,\n",
       " 3,\n",
       " 21,\n",
       " 18,\n",
       " 34,\n",
       " 38,\n",
       " 4,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 41,\n",
       " 17,\n",
       " 19,\n",
       " 12,\n",
       " 18,\n",
       " 34,\n",
       " 21,\n",
       " 34,\n",
       " 38,\n",
       " 57,\n",
       " 17,\n",
       " 62,\n",
       " 22,\n",
       " 56,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 38,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 42,\n",
       " 48,\n",
       " 21,\n",
       " 20,\n",
       " 17,\n",
       " 62,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 38,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 1,\n",
       " 34,\n",
       " 18,\n",
       " 34,\n",
       " 54,\n",
       " 56,\n",
       " 34,\n",
       " 38,\n",
       " 58,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 38,\n",
       " 46,\n",
       " 54,\n",
       " 29,\n",
       " 48,\n",
       " 54,\n",
       " 62,\n",
       " 27,\n",
       " 38,\n",
       " 16,\n",
       " 32,\n",
       " 32,\n",
       " 61,\n",
       " 38,\n",
       " 52,\n",
       " 49,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 38,\n",
       " 16,\n",
       " 28,\n",
       " 28,\n",
       " 15,\n",
       " 54,\n",
       " 56,\n",
       " 21,\n",
       " 38,\n",
       " 5,\n",
       " 12,\n",
       " 59,\n",
       " 54,\n",
       " 21,\n",
       " 34,\n",
       " 59,\n",
       " 38,\n",
       " 8,\n",
       " 17,\n",
       " 14,\n",
       " 34,\n",
       " 19,\n",
       " 2,\n",
       " 34,\n",
       " 62,\n",
       " 38,\n",
       " 7,\n",
       " 38,\n",
       " 50,\n",
       " 28,\n",
       " 16,\n",
       " 32,\n",
       " 15,\n",
       " 54,\n",
       " 29,\n",
       " 37,\n",
       " 48,\n",
       " 54,\n",
       " 37,\n",
       " 34,\n",
       " 38,\n",
       " 52,\n",
       " 29,\n",
       " 37,\n",
       " 18,\n",
       " 3,\n",
       " 56,\n",
       " 20,\n",
       " 41,\n",
       " 20,\n",
       " 54,\n",
       " 62,\n",
       " 54,\n",
       " 0,\n",
       " 21,\n",
       " 34,\n",
       " 62,\n",
       " 38,\n",
       " 56,\n",
       " 34,\n",
       " 21,\n",
       " 38,\n",
       " 34,\n",
       " 29,\n",
       " 0,\n",
       " 17,\n",
       " 59,\n",
       " 3,\n",
       " 29,\n",
       " 37,\n",
       " 38,\n",
       " 5,\n",
       " 4,\n",
       " 35,\n",
       " 44,\n",
       " 38,\n",
       " 25,\n",
       " 4,\n",
       " 42,\n",
       " 1,\n",
       " 4,\n",
       " 38,\n",
       " 45,\n",
       " 35,\n",
       " 38,\n",
       " 4,\n",
       " 30,\n",
       " 53,\n",
       " 25,\n",
       " 38,\n",
       " 26,\n",
       " 1,\n",
       " 45,\n",
       " 46,\n",
       " 52,\n",
       " 41,\n",
       " 4,\n",
       " 38,\n",
       " 36,\n",
       " 5,\n",
       " 4,\n",
       " 52,\n",
       " 8,\n",
       " 49,\n",
       " 52,\n",
       " 1,\n",
       " 36,\n",
       " 38,\n",
       " 52,\n",
       " 49,\n",
       " 45,\n",
       " 45,\n",
       " 9,\n",
       " 38,\n",
       " 4,\n",
       " 30,\n",
       " 52,\n",
       " 38,\n",
       " 41,\n",
       " 45,\n",
       " 6,\n",
       " 26,\n",
       " 15,\n",
       " 52,\n",
       " 4,\n",
       " 52,\n",
       " 38,\n",
       " 57,\n",
       " 45,\n",
       " 1,\n",
       " 9,\n",
       " 25,\n",
       " 38,\n",
       " 45,\n",
       " 35,\n",
       " 38,\n",
       " 57,\n",
       " 53,\n",
       " 15,\n",
       " 15,\n",
       " 53,\n",
       " 42,\n",
       " 6,\n",
       " 38,\n",
       " 25,\n",
       " 30,\n",
       " 42,\n",
       " 9,\n",
       " 52,\n",
       " 25,\n",
       " 26,\n",
       " 52,\n",
       " 42,\n",
       " 1,\n",
       " 52,\n",
       " 38,\n",
       " 4,\n",
       " 20,\n",
       " 34,\n",
       " 38,\n",
       " 41,\n",
       " 17,\n",
       " 19,\n",
       " 12,\n",
       " 18,\n",
       " 34,\n",
       " 21,\n",
       " 34,\n",
       " 38,\n",
       " 57,\n",
       " 17,\n",
       " 62,\n",
       " 22,\n",
       " 56,\n",
       " 38,\n",
       " 17,\n",
       " 23,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 38,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 2,\n",
       " 27,\n",
       " 38,\n",
       " 57,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 19,\n",
       " 38,\n",
       " 25,\n",
       " 20,\n",
       " 54,\n",
       " 22,\n",
       " 34,\n",
       " 56,\n",
       " 12,\n",
       " 34,\n",
       " 54,\n",
       " 62,\n",
       " 34,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 41,\n",
       " 17,\n",
       " 29,\n",
       " 21,\n",
       " 34,\n",
       " 29,\n",
       " 21,\n",
       " 56,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 4,\n",
       " 30,\n",
       " 52,\n",
       " 38,\n",
       " 25,\n",
       " 45,\n",
       " 8,\n",
       " 8,\n",
       " 52,\n",
       " 4,\n",
       " 25,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 42,\n",
       " 15,\n",
       " 15,\n",
       " 25,\n",
       " 38,\n",
       " 57,\n",
       " 52,\n",
       " 15,\n",
       " 15,\n",
       " 38,\n",
       " 4,\n",
       " 30,\n",
       " 42,\n",
       " 4,\n",
       " 38,\n",
       " 52,\n",
       " 8,\n",
       " 58,\n",
       " 25,\n",
       " 38,\n",
       " 57,\n",
       " 52,\n",
       " 15,\n",
       " 15,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 4,\n",
       " 30,\n",
       " 52,\n",
       " 38,\n",
       " 4,\n",
       " 1,\n",
       " 42,\n",
       " 36,\n",
       " 52,\n",
       " 58,\n",
       " 39,\n",
       " 38,\n",
       " 45,\n",
       " 35,\n",
       " 38,\n",
       " 42,\n",
       " 8,\n",
       " 4,\n",
       " 45,\n",
       " 8,\n",
       " 39,\n",
       " 38,\n",
       " 42,\n",
       " 8,\n",
       " 58,\n",
       " 38,\n",
       " 41,\n",
       " 15,\n",
       " 52,\n",
       " 45,\n",
       " 26,\n",
       " 42,\n",
       " 4,\n",
       " 1,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 42,\n",
       " 25,\n",
       " 38,\n",
       " 39,\n",
       " 45,\n",
       " 5,\n",
       " 38,\n",
       " 15,\n",
       " 53,\n",
       " 9,\n",
       " 52,\n",
       " 38,\n",
       " 53,\n",
       " 4,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1037905, 30, 63)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X[i, t, char] = 1\n",
    "    y[i, next_char[i]] = 1\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 63)                32004     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 63)                4032      \n",
      "=================================================================\n",
      "Total params: 36,036\n",
      "Trainable params: 36,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(len(chars), input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, c_to_i[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = i_to_c[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1037905 samples\n",
      "Epoch 1/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 2.2824\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ll never hold that man my frie\"\n",
      "ll never hold that man my fries  JUIVANGUSS Orothim of to fors hish What dutsons you thy some for thourd a on andies far scoode    wirguod thim     Bure bot be fif goe noY hil shouny teres    ITht ang cnakice dringRangure is eriss deow he home lives and gere coth or hat      ang somandOO Castary Thas Fomest yiu gureeOThat the of bodest thig baghy the afe he of feencbeld anvor wemaruing    Wiylat be thie Voll dle gigeende thou \n",
      "1037905/1037905 [==============================] - 313s 302us/sample - loss: 2.2824\n",
      "Epoch 2/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 2.0014\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"d room enoughWhen there is in \"\n",
      "d room enoughWhen there is in mansirk wrange    Andreon Bensade weld lasse SDOUK These workned andNome grod the  speensed of    Letheed eves where cif thee penderThe our not is    If gof           KICCAND PORT    swear much aud falf our   VAnd cackcould shinglape they lold well thee they mid be offure your Dlaysion Joon arlsaofed and drubt good ap apunt in un ut mowed    For ither well hon jorger a diedatHeve for a dears growl\n",
      "1037905/1037905 [==============================] - 312s 301us/sample - loss: 2.0014\n",
      "Epoch 3/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 1.9145\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"n nor Sir Guy nor Colbrand    \"\n",
      "n nor Sir Guy nor Colbrand    Lall with I telds them fill ig with depoll be upfition till a evd onelant us do a lacleStangCole your whandArnis thuse tithelyKISs wrom hhawA tounts is three For sneetcrethig as a tuth he jear makest you veralidMTiment our to net go hish    Deainded your it splistouldOLLUD Whappriit to perainTRIMONA Os hiswHoCKI GANMORDLLUS Fit hiess   A surpess renowronk Radies net tigeTIm mounds    OusOfs carine\n",
      "1037905/1037905 [==============================] - 336s 323us/sample - loss: 1.9145\n",
      "Epoch 4/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 1.8632\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"asher and Servants attending o\"\n",
      "asher and Servants attending of in is and sweer Chand oude hall so my plaged purior Quent what to Missard Is buthand    Othis hears heave you then featiin ugcy be mect ithing Mast him have Verilatoundander then now for when be tis a for the comsies repirais this oof is with a more we some mast tros I wairer The in the rike your thousce comseThat to paire    seed  AN that at mand and dgain     in thus with conaltreThou we thou \n",
      "1037905/1037905 [==============================] - 334s 322us/sample - loss: 1.8632\n",
      "Epoch 5/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 1.8274\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ing in his face    Not able to\"\n",
      "ing in his face    Not able told thee fate the gear that spert spound un ventluce oft for sown the last     thishalm corether fishle speays our acdMiabulAs mundand hat your have or repically fill ap aben ane suble  QUCOLILA Avinto Hen marrute theYesing a gatten   MasterBASSER I coargen theance at igrOVEALE The mour weild    But loals of hearss have and uporses in thousd faterir in this loths Mangeledt   Awath woild the sut ref\n",
      "1037905/1037905 [==============================] - 381s 367us/sample - loss: 1.8274\n",
      "Epoch 6/10\n",
      "1037696/1037905 [============================>.] - ETA: 0s - loss: 1.8008\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"nd of art     Heaven and our L\"\n",
      "nd of art     Heaven and our Lonchoched be for heaving to this park to    Youghter A9But Ithouse and renem as     Hims suest of sein be broove fourn    Yes with a vill nevily tawes masing redcuriss in of nightsAnd owd She mavery I will ADo sallGid    What of KInTIMON TIAGTRATAS Tilene with fagaund so more houssableice righton the sillend and suck Pefulal all thar shojescin in You yeneeddinewill Coges take ippinite befeaded is \n",
      "1037905/1037905 [==============================] - 398s 384us/sample - loss: 1.8007\n",
      "Epoch 7/10\n",
      "1037824/1037905 [============================>.] - ETA: 0s - loss: 1.7800\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \" as best becomes a gentlewoman\"\n",
      " as best becomes a gentlewomann I would to and in      GENDULE So all Due I have plavery bray me I frunce our whan at gods theleFEONO Cad your land   Suthars dealm hinksterone   Exit you Inders benoul sabld full prosechar timen jupts dieTodeath liverThe geardus    Uneradys fartary surtA let not conshelf with stake Capceand and now day Ile befander supan and rights would be gle our with and timant that sour will    And what han\n",
      "1037905/1037905 [==============================] - 361s 348us/sample - loss: 1.7799\n",
      "Epoch 8/10\n",
      "1011200/1037905 [============================>.] - ETA: 7s - loss: 1.7634"
     ]
    }
   ],
   "source": [
    "import random, sys\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=10, callbacks=[print_callback],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cba150e94c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m unicorns = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "unicorns = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size, \n",
    "    epochs=5,\n",
    "    callbacks=[print_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
